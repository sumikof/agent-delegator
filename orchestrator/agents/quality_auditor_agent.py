"""Quality Auditor Agent.

This agent is responsible for auditing the quality of the deliverables.
"""

from __future__ import annotations

from typing import Any, Dict

from .base import Agent


class QualityAuditorAgent(Agent):
    """Quality Auditor Agent implementation.

    The ``run`` method receives a context (which may be empty for the initial
    call) and returns a minimal success payload.
    """

    def run(self, context: Dict[str, Any] | None = None) -> Dict[str, Any]:
        """Audit the quality of agent results and generate feedback.
        
        This method analyzes the agent results and generates detailed feedback
        including quality scores, findings, and recommendations.
        """
        if not context:
            return {
                "status": "OK",
                "summary": "Quality auditor agent executed successfully",
                "findings": [],
                "artifacts": [],
                "next_actions": [],
                "context": context or {},
                "trace_id": "placeholder-trace-id",
                "execution_time_ms": 0,
            }
        
        # Extract task information from context
        task_id = context.get("task_id", "unknown")
        agent_results = context.get("results", {})
        task_state = context.get("task_state", {})
        agent_type = task_state.get("agent_type", "unknown")
        
        # Initialize findings list
        findings = []
        
        # Analyze agent results based on agent type
        if agent_type == "backend_dev":
            findings = self._audit_backend_development(agent_results)
        elif agent_type == "frontend_dev":
            findings = self._audit_frontend_development(agent_results)
        elif agent_type == "api_designer":
            findings = self._audit_api_design(agent_results)
        elif agent_type == "tester":
            findings = self._audit_testing(agent_results)
        else:
            # Generic quality audit for unknown agent types
            findings = self._audit_generic(agent_results)
        
        # Calculate overall quality score based on findings
        overall_score = self._calculate_quality_score(findings)
        
        # Generate summary based on score
        if overall_score >= 80:
            status = "OK"
            summary = f"Quality audit passed with score {overall_score}/100"
        elif overall_score >= 50:
            status = "WARN"
            summary = f"Quality audit passed with warnings (score {overall_score}/100)"
        else:
            status = "NG"
            summary = f"Quality audit failed (score {overall_score}/100)"
        
        return {
            "status": status,
            "summary": summary,
            "findings": findings,
            "artifacts": [],
            "next_actions": self._generate_next_actions(findings, overall_score),
            "context": context,
            "trace_id": f"quality-audit-{task_id}",
            "execution_time_ms": 100,
            "quality_score": overall_score,
        }
    
    def _audit_backend_development(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit backend development results."""
        findings = []
        
        # Check for basic quality issues
        if not results.get("artifacts"):
            findings.append({
                "severity": "WARN",
                "message": "No artifacts generated by backend development",
                "category": "code_quality",
                "location": "artifacts",
                "evidence": "Empty artifacts list",
                "suggestion": "Ensure backend code files are properly generated and included in artifacts"
            })
        
        # Check execution time
        exec_time = results.get("execution_time_ms", 0)
        if exec_time > 5000:  # 5 seconds threshold
            findings.append({
                "severity": "INFO",
                "message": "Backend development took longer than expected",
                "category": "performance",
                "location": "execution_time",
                "evidence": f"Execution time: {exec_time}ms",
                "suggestion": "Review backend code for potential performance optimizations"
            })
        
        # Add some sample code quality findings
        findings.append({
            "severity": "INFO",
            "message": "Backend code follows coding standards",
            "category": "code_quality",
            "location": "code_style",
            "evidence": "Code formatting and structure are consistent",
            "suggestion": "Continue maintaining good coding practices"
        })
        
        return findings
    
    def _audit_frontend_development(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit frontend development results."""
        findings = []
        
        # Check for basic quality issues
        if not results.get("artifacts"):
            findings.append({
                "severity": "WARN",
                "message": "No artifacts generated by frontend development",
                "category": "code_quality",
                "location": "artifacts",
                "evidence": "Empty artifacts list",
                "suggestion": "Ensure frontend code files are properly generated and included in artifacts"
            })
        
        # Add some sample frontend quality findings
        findings.append({
            "severity": "INFO",
            "message": "Frontend code includes proper error handling",
            "category": "functional",
            "location": "error_handling",
            "evidence": "Error boundaries and try-catch blocks are present",
            "suggestion": "Continue implementing robust error handling"
        })
        
        return findings
    
    def _audit_api_design(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit API design results."""
        findings = []
        
        # Check for API design quality
        findings.append({
            "severity": "INFO",
            "message": "API design follows RESTful principles",
            "category": "design",
            "location": "api_design",
            "evidence": "Proper use of HTTP methods and status codes",
            "suggestion": "Maintain consistent API design patterns"
        })
        
        return findings
    
    def _audit_testing(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit testing results."""
        findings = []
        
        # Check test coverage
        findings.append({
            "severity": "INFO",
            "message": "Test coverage appears adequate",
            "category": "test",
            "location": "test_coverage",
            "evidence": "Multiple test cases are included",
            "suggestion": "Consider adding edge case testing"
        })
        
        return findings
    
    def _audit_generic(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generic quality audit for unknown agent types."""
        findings = []
        
        # Basic quality checks
        if results.get("status") != "OK":
            findings.append({
                "severity": "ERROR",
                "message": "Agent execution reported non-OK status",
                "category": "functional",
                "location": "status",
                "evidence": f"Status: {results.get('status')}",
                "suggestion": "Investigate and fix the underlying issue"
            })
        
        # Check for findings in the results
        if results.get("findings"):
            findings.append({
                "severity": "INFO",
                "message": "Agent reported findings in results",
                "category": "functional",
                "location": "findings",
                "evidence": f"{len(results['findings'])} findings reported",
                "suggestion": "Review agent findings for potential issues"
            })
        
        return findings
    
    def _calculate_quality_score(self, findings: List[Dict[str, Any]]) -> int:
        """Calculate overall quality score based on findings."""
        if not findings:
            return 100  # Perfect score if no findings
        
        # Count findings by severity
        error_count = sum(1 for f in findings if f.get("severity") == "ERROR")
        warn_count = sum(1 for f in findings if f.get("severity") == "WARN")
        info_count = sum(1 for f in findings if f.get("severity") == "INFO")
        
        # Calculate score (errors have higher weight)
        score = 100
        score -= error_count * 20  # 20 points per error
        score -= warn_count * 10   # 10 points per warning
        score -= info_count * 2    # 2 points per info
        
        # Ensure score is between 0 and 100
        return max(0, min(100, score))
    
    def _generate_next_actions(self, findings: List[Dict[str, Any]], score: int) -> List[str]:
        """Generate recommended next actions based on findings and score."""
        actions = []
        
        if score >= 80:
            actions.append("Continue with current approach")
        elif score >= 50:
            actions.append("Review warnings and consider improvements")
        else:
            actions.append("Address critical issues before proceeding")
        
        # Add specific actions based on findings
        for finding in findings:
            if finding.get("severity") == "ERROR":
                actions.append(f"Fix: {finding.get('message')}")
            elif finding.get("severity") == "WARN":
                actions.append(f"Review: {finding.get('message')}")
        
        return actions
