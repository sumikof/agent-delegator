"""Quality Auditor Agent.

This agent is responsible for auditing the quality of the deliverables.
"""

from __future__ import annotations

import re
from typing import Any, Dict, List

from .base import Agent


class QualityAuditorAgent(Agent):
    """Quality Auditor Agent implementation.

    The ``run`` method receives a context (which may be empty for the initial
    call) and returns a minimal success payload.
    """

    def run(self, context: Dict[str, Any] | None = None) -> Dict[str, Any]:
        """Audit the quality of agent results and generate feedback.
        
        This method analyzes the agent results and generates detailed feedback
        including quality scores, findings, and recommendations.
        """
        if not context:
            return {
                "status": "OK",
                "summary": "Quality auditor agent executed successfully",
                "findings": [],
                "artifacts": [],
                "next_actions": [],
                "context": context or {},
                "trace_id": "placeholder-trace-id",
                "execution_time_ms": 0,
            }
        
        # Extract task information from context
        task_id = context.get("task_id", "unknown")
        agent_results = context.get("results", {})
        task_state = context.get("task_state", {})
        agent_type = task_state.get("agent_type", "unknown")
        
        # Initialize findings list
        findings = []
        
        # Analyze agent results based on agent type
        if agent_type == "backend_dev":
            findings = self._audit_backend_development(agent_results)
        elif agent_type == "frontend_dev":
            findings = self._audit_frontend_development(agent_results)
        elif agent_type == "api_designer":
            findings = self._audit_api_design(agent_results)
        elif agent_type == "tester":
            findings = self._audit_testing(agent_results)
        else:
            # Generic quality audit for unknown agent types
            findings = self._audit_generic(agent_results)
        
        # Calculate overall quality score based on findings
        overall_score = self._calculate_quality_score(findings)
        
        # Generate summary based on score
        if overall_score >= 80:
            status = "OK"
            summary = f"Quality audit passed with score {overall_score}/100"
        elif overall_score >= 50:
            status = "WARN"
            summary = f"Quality audit passed with warnings (score {overall_score}/100)"
        else:
            status = "NG"
            summary = f"Quality audit failed (score {overall_score}/100)"
        
        return {
            "status": status,
            "summary": summary,
            "findings": findings,
            "artifacts": [],
            "next_actions": self._generate_next_actions(findings, overall_score),
            "context": context,
            "trace_id": f"quality-audit-{task_id}",
            "execution_time_ms": 100,
            "quality_score": overall_score,
        }
    
    def _audit_backend_development(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit backend development results with comprehensive quality checks."""
        findings = []
        
        # Check for basic quality issues
        if not results.get("artifacts"):
            findings.append({
                "severity": "ERROR",
                "message": "No artifacts generated by backend development",
                "category": "code_quality",
                "location": "artifacts",
                "evidence": "Empty artifacts list",
                "suggestion": "Ensure backend code files are properly generated and included in artifacts"
            })
            return findings
        
        # Analyze artifacts for code quality
        artifacts = results.get("artifacts", [])
        code_files = [a for a in artifacts if a.get("type") == "code"]
        
        if not code_files:
            findings.append({
                "severity": "ERROR",
                "message": "No code artifacts found in backend development results",
                "category": "code_quality",
                "location": "artifacts",
                "evidence": "No code files in artifacts",
                "suggestion": "Ensure backend code files are properly categorized as 'code' type"
            })
            return findings
        
        # Check for common code quality issues
        for artifact in code_files:
            if "path" in artifact and artifact["path"].endswith(".py"):
                # Check for Python code quality issues
                findings.extend(self._audit_python_code(artifact))
            elif "path" in artifact and artifact["path"].endswith(".js"):
                # Check for JavaScript code quality issues
                findings.extend(self._audit_javascript_code(artifact))
        
        # Check execution time
        exec_time = results.get("execution_time_ms", 0)
        if exec_time > 10000:  # 10 seconds threshold
            findings.append({
                "severity": "WARN",
                "message": "Backend development took significantly longer than expected",
                "category": "performance",
                "location": "execution_time",
                "evidence": f"Execution time: {exec_time}ms",
                "suggestion": "Review backend code for potential performance optimizations and consider caching strategies"
            })
        elif exec_time > 5000:  # 5 seconds warning threshold
            findings.append({
                "severity": "INFO",
                "message": "Backend development took longer than expected",
                "category": "performance",
                "location": "execution_time",
                "evidence": f"Execution time: {exec_time}ms",
                "suggestion": "Review backend code for potential optimizations"
            })
        
        # Check for documentation
        doc_files = [a for a in artifacts if a.get("type") == "doc"]
        if not doc_files:
            findings.append({
                "severity": "WARN",
                "message": "No documentation artifacts found for backend code",
                "category": "documentation",
                "location": "artifacts",
                "evidence": "No documentation files in artifacts",
                "suggestion": "Add API documentation, code comments, and architecture diagrams"
            })
        
        # Check for test files
        test_files = [a for a in artifacts if a.get("type") == "test"]
        if not test_files:
            findings.append({
                "severity": "ERROR",
                "message": "No test artifacts found for backend code",
                "category": "test",
                "location": "artifacts",
                "evidence": "No test files in artifacts",
                "suggestion": "Implement comprehensive unit tests, integration tests, and API tests"
            })
        elif len(test_files) < len(code_files) * 0.5:  # At least 50% test coverage
            findings.append({
                "severity": "WARN",
                "message": "Insufficient test coverage for backend code",
                "category": "test",
                "location": "artifacts",
                "evidence": f"Only {len(test_files)} test files for {len(code_files)} code files",
                "suggestion": "Increase test coverage to at least 50% of code files"
            })
        
        return findings
    
    def _audit_python_code(self, artifact: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit Python code quality."""
        findings = []
        
        # Check for common Python code quality issues
        if "content" in artifact:
            content = artifact["content"]
            
            # Check for hardcoded credentials
            credential_patterns = [
                r"password\s*=\s*['\"][^'\"]+['\"]",
                r"secret\s*=\s*['\"][^'\"]+['\"]",
                r"api_key\s*=\s*['\"][^'\"]+['\"]",
                r"token\s*=\s*['\"][^'\"]+['\"]"
            ]
            
            for pattern in credential_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    findings.append({
                        "severity": "CRITICAL",
                        "message": "Hardcoded credentials found in code",
                        "category": "security",
                        "location": artifact.get("path", "unknown"),
                        "evidence": "Hardcoded credentials detected",
                        "suggestion": "Use environment variables or secret management system for credentials"
                    })
                    break
            
            # Check for print statements (potential debug code)
            if re.search(r"print\s*\(", content):
                findings.append({
                    "severity": "WARN",
                    "message": "Print statements found in code",
                    "category": "code_quality",
                    "location": artifact.get("path", "unknown"),
                    "evidence": "Print statements detected",
                    "suggestion": "Use proper logging instead of print statements"
                })
            
            # Check for broad exception handling
            if re.search(r"except:\s*pass", content):
                findings.append({
                    "severity": "ERROR",
                    "message": "Empty except block found",
                    "category": "error_handling",
                    "location": artifact.get("path", "unknown"),
                    "evidence": "Empty except block detected",
                    "suggestion": "Handle exceptions properly or use specific exception types"
                })
            
            # Check for proper docstrings
            if not re.search(r'""".*"""', content, re.DOTALL) and not re.search(r"'''.*'''", content, re.DOTALL):
                findings.append({
                    "severity": "INFO",
                    "message": "Missing docstrings in code",
                    "category": "documentation",
                    "location": artifact.get("path", "unknown"),
                    "evidence": "No docstrings found",
                    "suggestion": "Add proper docstrings for functions and classes"
                })
            
            # Check for long functions (more than 50 lines)
            lines = content.split('\n')
            function_line_count = 0
            in_function = False
            
            for line in lines:
                if re.match(r"\s*def\s+\w+", line):
                    in_function = True
                    function_line_count = 1
                elif in_function:
                    function_line_count += 1
                    if function_line_count > 50:
                        findings.append({
                            "severity": "WARN",
                            "message": "Long function detected (more than 50 lines)",
                            "category": "code_quality",
                            "location": artifact.get("path", "unknown"),
                            "evidence": f"Function with {function_line_count} lines",
                            "suggestion": "Break down long functions into smaller, focused functions"
                        })
                        break
                    elif line.strip() and not line.strip().startswith('#') and re.match(r"\s*def\s+\w+", line):
                        in_function = False
        
        return findings
    
    def _audit_javascript_code(self, artifact: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit JavaScript code quality."""
        findings = []
        
        # Check for common JavaScript code quality issues
        if "content" in artifact:
            content = artifact["content"]
            
            # Check for console.log statements (potential debug code)
            if re.search(r"console\.log\s*\(", content):
                findings.append({
                    "severity": "WARN",
                    "message": "Console.log statements found in code",
                    "category": "code_quality",
                    "location": artifact.get("path", "unknown"),
                    "evidence": "Console.log statements detected",
                    "suggestion": "Use proper logging library instead of console.log"
                })
            
            # Check for hardcoded credentials
            credential_patterns = [
                r"password\s*=\s*['\"][^'\"]+['\"]",
                r"secret\s*=\s*['\"][^'\"]+['\"]",
                r"apiKey\s*=\s*['\"][^'\"]+['\"]",
                r"token\s*=\s*['\"][^'\"]+['\"]"
            ]
            
            for pattern in credential_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    findings.append({
                        "severity": "CRITICAL",
                        "message": "Hardcoded credentials found in code",
                        "category": "security",
                        "location": artifact.get("path", "unknown"),
                        "evidence": "Hardcoded credentials detected",
                        "suggestion": "Use environment variables or secret management system for credentials"
                    })
                    break
            
            # Check for proper error handling
            if not re.search(r"try\s*\{", content):
                findings.append({
                    "severity": "INFO",
                    "message": "No try-catch blocks found in code",
                    "category": "error_handling",
                    "location": artifact.get("path", "unknown"),
                    "evidence": "No error handling detected",
                    "suggestion": "Add proper error handling with try-catch blocks"
                })
            
            # Check for async/await usage in modern JS
            if re.search(r"async\s+function", content) and not re.search(r"await\s+\w+", content):
                findings.append({
                    "severity": "INFO",
                    "message": "Async function without await detected",
                    "category": "code_quality",
                    "location": artifact.get("path", "unknown"),
                    "evidence": "Async function without await",
                    "suggestion": "Ensure async functions use await properly"
                })
        
        return findings
    
    def _audit_frontend_development(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit frontend development results."""
        findings = []
        
        # Check for basic quality issues
        if not results.get("artifacts"):
            findings.append({
                "severity": "WARN",
                "message": "No artifacts generated by frontend development",
                "category": "code_quality",
                "location": "artifacts",
                "evidence": "Empty artifacts list",
                "suggestion": "Ensure frontend code files are properly generated and included in artifacts"
            })
        
        # Add some sample frontend quality findings
        findings.append({
            "severity": "INFO",
            "message": "Frontend code includes proper error handling",
            "category": "functional",
            "location": "error_handling",
            "evidence": "Error boundaries and try-catch blocks are present",
            "suggestion": "Continue implementing robust error handling"
        })
        
        return findings
    
    def _audit_api_design(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit API design results."""
        findings = []
        
        # Check for API design quality
        findings.append({
            "severity": "INFO",
            "message": "API design follows RESTful principles",
            "category": "design",
            "location": "api_design",
            "evidence": "Proper use of HTTP methods and status codes",
            "suggestion": "Maintain consistent API design patterns"
        })
        
        return findings
    
    def _audit_testing(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Audit testing results."""
        findings = []
        
        # Check test coverage
        findings.append({
            "severity": "INFO",
            "message": "Test coverage appears adequate",
            "category": "test",
            "location": "test_coverage",
            "evidence": "Multiple test cases are included",
            "suggestion": "Consider adding edge case testing"
        })
        
        return findings
    
    def _audit_generic(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generic quality audit for unknown agent types."""
        findings = []
        
        # Basic quality checks
        if results.get("status") != "OK":
            findings.append({
                "severity": "ERROR",
                "message": "Agent execution reported non-OK status",
                "category": "functional",
                "location": "status",
                "evidence": f"Status: {results.get('status')}",
                "suggestion": "Investigate and fix the underlying issue"
            })
        
        # Check for findings in the results
        if results.get("findings"):
            findings.append({
                "severity": "INFO",
                "message": "Agent reported findings in results",
                "category": "functional",
                "location": "findings",
                "evidence": f"{len(results['findings'])} findings reported",
                "suggestion": "Review agent findings for potential issues"
            })
        
        return findings
    
    def _calculate_quality_score(self, findings: List[Dict[str, Any]]) -> int:
        """Calculate overall quality score based on findings."""
        if not findings:
            return 100  # Perfect score if no findings
        
        # Count findings by severity
        error_count = sum(1 for f in findings if f.get("severity") == "ERROR")
        warn_count = sum(1 for f in findings if f.get("severity") == "WARN")
        info_count = sum(1 for f in findings if f.get("severity") == "INFO")
        
        # Calculate score (errors have higher weight)
        score = 100
        score -= error_count * 20  # 20 points per error
        score -= warn_count * 10   # 10 points per warning
        score -= info_count * 2    # 2 points per info
        
        # Ensure score is between 0 and 100
        return max(0, min(100, score))
    
    def _generate_next_actions(self, findings: List[Dict[str, Any]], score: int) -> List[str]:
        """Generate recommended next actions based on findings and score."""
        actions = []
        
        if score >= 80:
            actions.append("Continue with current approach")
        elif score >= 50:
            actions.append("Review warnings and consider improvements")
        else:
            actions.append("Address critical issues before proceeding")
        
        # Add specific actions based on findings
        for finding in findings:
            if finding.get("severity") == "ERROR":
                actions.append(f"Fix: {finding.get('message')}")
            elif finding.get("severity") == "WARN":
                actions.append(f"Review: {finding.get('message')}")
        
        return actions
